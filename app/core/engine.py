import os
import logging
import subprocess
import uuid
import re


# ç®€å•çš„é…ç½®ç±»
class SimpleConfig:
    def __init__(self, dictionary):
        for key, value in dictionary.items():
            if isinstance(value, dict):
                value = SimpleConfig(value)
            setattr(self, key, value)


class ELE_Service:
    def __init__(self, task_config: dict, llm_client, base_temp_dir: str = "/tmp/mela_tasks"):
        self.cfg = SimpleConfig(task_config)
        self.llm = llm_client  # è¿™é‡Œå°±æ˜¯ llm.py é‡Œçš„ llm_service
        self.task_id = str(uuid.uuid4())
        logging.basicConfig(level=logging.INFO)

    def _extract_code(self, llm_response: str) -> str:
        """
        ä» LLM çš„å›å¤ä¸­æå– ```python ... ``` ä¹‹é—´çš„ä»£ç 
        """
        # ä½¿ç”¨æ­£åˆ™æå– Markdown ä»£ç å—
        match = re.search(r"```python(.*?)```", llm_response, re.DOTALL)
        if match:
            return match.group(1).strip()

        # å…œåº•ï¼šå¦‚æœæ²¡æ‰¾åˆ° python æ ‡ç­¾ï¼Œå°è¯•æ‰¾é€šç”¨ä»£ç å—
        match_general = re.search(r"```(.*?)```", llm_response, re.DOTALL)
        if match_general:
            return match_general.group(1).strip()

        return llm_response.replace("```", "").strip()

    def _generate_code_with_llm(self, query: str) -> str:
        """
        è®© DeepSeek ç¼–å†™è§£å†³é—®é¢˜çš„ Python ä»£ç 
        """
        # 1. å®šä¹‰ System Prompt (äººè®¾)
        sys_prompt = "ä½ æ˜¯ä¸€ä¸ª Python ç¼–ç¨‹ä¸“å®¶ã€‚åªè¿”å›ä»£ç ï¼Œä¸è¦è§£é‡Šã€‚"

        # 2. å®šä¹‰ User Prompt (å…·ä½“è¦æ±‚)
        user_prompt = f"""
        è¯·ç¼–å†™ä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬æ¥è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
        "{query}"

        è¦æ±‚ï¼š
        1. ä»£ç å¿…é¡»æ˜¯å®Œæ•´çš„ã€å¯è¿è¡Œçš„ã€‚
        2. å¿…é¡»å°†æœ€ç»ˆç»“æœé€šè¿‡ print() æ‰“å°åˆ°æ§åˆ¶å°ã€‚
        3. ä¸è¦ä½¿ç”¨ input() ç­‰å¾…ç”¨æˆ·è¾“å…¥ã€‚
        4. å¼•å…¥å¿…è¦çš„åº“ï¼ˆå¦‚ math, random ç­‰ï¼‰ã€‚
        5. ä»£ç å¿…é¡»åŒ…è£¹åœ¨ ```python å’Œ ``` ä¹‹é—´ã€‚
        """

        logging.info(f"ğŸ¤– Asking DeepSeek to write code for: {query}")

        #  å…³é”®è°ƒç”¨ï¼šä½¿ç”¨æˆ‘ä»¬åœ¨ llm.py æ–°å¢çš„ chat æ–¹æ³•
        response = self.llm.chat(prompt=user_prompt, system_prompt=sys_prompt)

        return self._extract_code(response)

    def _run_code_in_docker(self, code_content: str):
        """
        æµå¼æ³¨å…¥ä»£ç åˆ° Docker å®¹å™¨
        """
        cmd = [
            "docker", "run", "--rm", "-i", "--network", "none",
            "--cpus", "1.0", "--memory", "512m",
            "python:3.9-slim", "python", "-"
        ]
        logging.info(f"Sandbox Execution: {' '.join(cmd)}")
        try:
            # input=code_content æ˜¯æ ¸å¿ƒï¼Œç›´æ¥æŠŠä»£ç å–‚ç»™ stdin
            result = subprocess.run(
                cmd, input=code_content, capture_output=True, text=True, timeout=60
            )
            if result.returncode == 0:
                return {"status": "success", "output": result.stdout}
            else:
                return {"status": "error", "error": result.stderr}
        except subprocess.TimeoutExpired:
            return {"status": "timeout", "error": "Code execution timed out"}
        except Exception as e:
            return {"status": "system_error", "error": str(e)}



    def run(self, query: str = "Solve TSP"):
            logging.info(f"Task {self.task_id} started. Query: {query}")

            # 1. çœŸÂ·LLM ä»£ç ç”Ÿæˆ
            try:
                generated_code = self._generate_code_with_llm(query)
                logging.info("Code generated successfully.")
            except Exception as e:
                logging.error(f"LLM Generation failed: {e}")
                return {"status": "llm_error", "error": str(e)}

            # 2. Docker æ‰§è¡Œ
            execution_result = self._run_code_in_docker(generated_code)

            # æŠŠç”Ÿæˆçš„ä»£ç ä¹Ÿæ”¾è¿›ç»“æœé‡Œï¼
            execution_result["generated_code"] = generated_code

            logging.info(f"Task finished. Result: {execution_result}")
            return execution_result