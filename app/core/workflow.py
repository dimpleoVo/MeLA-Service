import logging
from typing import Dict, Any
from app.core.llm import llm_service
from app.core.rag import rag_service
# å¼•å…¥æˆ‘ä»¬åœ¨ Module 1 å†™çš„å¼•æ“ (ä¸ºäº†è°ƒç”¨ run æ–¹æ³•)
from app.core.engine import ELE_Service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AgentState:
    """
    ç”¨äºåœ¨ä¸åŒæ­¥éª¤ä¹‹é—´ä¼ é€’æ•°æ®çš„"é»‘æ¿"
    """

    def __init__(self, query: str):
        self.query = query
        self.memory = []  # æ‰€æœ‰çš„å¯¹è¯å†å²
        self.current_step = "start"
        self.final_answer = ""


class MeLA_Workflow:
    def __init__(self):
        logger.info("Initializing Agent Workflow...")

    def router_node(self, state: AgentState) -> str:
        """
        ã€è·¯ç”±èŠ‚ç‚¹ã€‘
        åˆ¤æ–­ç”¨æˆ·çš„æ„å›¾ï¼šæ˜¯æƒ³èŠå¤©(Chat)ï¼Ÿè¿˜æ˜¯æƒ³ä¼˜åŒ–ä»£ç (Optimize)ï¼Ÿ
        """
        logger.info(" Agent is thinking (Routing)...")

        # ä½¿ç”¨ DeepSeek è¿›è¡Œæ„å›¾è¯†åˆ«
        prompt = f"""
        ç”¨æˆ·è¾“å…¥: "{state.query}"
        è¯·åˆ¤æ–­ç”¨æˆ·æ„å›¾ã€‚
        - å¦‚æœç”¨æˆ·æƒ³è§£å†³æ•°å­¦ä¼˜åŒ–é—®é¢˜ã€å†™ä»£ç ã€æ±‚è§£TSP/èƒŒåŒ…é—®é¢˜ï¼Œè¿”å› "OPTIMIZE"ã€‚
        - å¦‚æœç”¨æˆ·åªæ˜¯è¯¢é—®çŸ¥è¯†ã€å®šä¹‰æ¦‚å¿µæˆ–é—²èŠï¼Œè¿”å› "CHAT"ã€‚
        åªè¿”å›å•è¯ï¼Œä¸è¦æ ‡ç‚¹ã€‚
        """
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œç›´æ¥è°ƒ generateã€‚ç”Ÿäº§ç¯å¢ƒä¼šç”¨ Function Callingã€‚
        intent = llm_service.generate(prompt, context_chunks=[])

        if "OPTIMIZE" in intent.upper():
            return "node_optimizer"
        else:
            return "node_chat"

    def optimizer_node(self, state: AgentState):
        """
        ã€å·¥å…·èŠ‚ç‚¹ã€‘è°ƒç”¨ ELE å¼•æ“æ‰§è¡Œä¼˜åŒ–ä»»åŠ¡
        """
        logger.info(" Agent is using Tool: Optimization Engine...")

        # 1. åˆå§‹åŒ–ä¼˜åŒ–å¼•æ“ (è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€åŒ–äº†é…ç½®)
        task_config = {"problem": {"problem_name": "User_Task"}, "max_fe": 10}
        ele = ELE_Service(task_config, llm_client=llm_service)

        # 2. æ‰§è¡Œä»»åŠ¡ (Module 1 çš„æ ¸å¿ƒé€»è¾‘)
        # è¿™é‡Œä¼šè§¦å‘ Docker/Mock
        result = ele.run()

        # 3. æ›´æ–°çŠ¶æ€
        state.final_answer = f" ä¼˜åŒ–ä»»åŠ¡å·²å®Œæˆã€‚\nå¼•æ“è¿è¡Œç»“æœ: {result['output']}"
        return state

    def chat_node(self, state: AgentState):
        """
        ã€å¯¹è¯èŠ‚ç‚¹ã€‘è°ƒç”¨ RAG + LLM å›ç­”é—®é¢˜
        """
        logger.info("ğŸ’¬ Agent is chatting (RAG Mode)...")

        # 1. RAG æ£€ç´¢
        search_res = rag_service.search(state.query)
        docs = search_res["results"]

        # 2. LLM ç”Ÿæˆ
        answer = llm_service.generate(state.query, context_chunks=docs)

        state.final_answer = answer
        return state

    def run(self, query: str):
        """
        ã€å›¾æ‰§è¡Œå¼•æ“ã€‘æ¨¡æ‹Ÿ LangGraph çš„è¿è¡Œé€»è¾‘
        Start -> Router -> (Optimizer / Chat) -> End
        """
        state = AgentState(query)

        # 1. è·¯ç”±é˜¶æ®µ
        next_step = self.router_node(state)

        # 2. æ‰§è¡Œé˜¶æ®µ
        if next_step == "node_optimizer":
            self.optimizer_node(state)
        elif next_step == "node_chat":
            self.chat_node(state)

        return state.final_answer


# å•ä¾‹
agent_workflow = MeLA_Workflow()